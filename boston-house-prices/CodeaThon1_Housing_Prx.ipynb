{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#a definition of the column names\n",
    "\"\"\"    - CRIM     per capita crime rate by town\n",
    "    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    - INDUS    proportion of non-retail business acres per town\n",
    "    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) ###note, a factor shouldn't be numeric\n",
    "    - NOX      nitric oxides concentration (parts per 10 million)\n",
    "    - RM       average number of rooms per dwelling\n",
    "    - AGE      proportion of owner-occupied units built prior to 1940\n",
    "    - DIS      weighted distances to five Boston employment centres\n",
    "    - RAD      index of accessibility to radial highways                             ###another category\n",
    "    - TAX      full-value property-tax rate per $10,000\n",
    "    - PTRATIO  pupil-teacher ratio by town\n",
    "    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    - LSTAT    % lower status of the population\n",
    "    - MEDV     Median value of owner-occupied homes in $1000's\"\"\"\n",
    "    \n",
    "def process_data(data):\n",
    "    return(data)\n",
    "\n",
    "data = pd.read_csv(\"housing.csv\",sep=\"\\s+\")\n",
    "data.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#more data processing, shows there are no null values\n",
    "basic_info = pd.concat([data.min(), data.max(), data.isnull().sum()], axis = 1)\n",
    "basic_info.columns = [\"Min\", \"Max\", \"# Null\"]\n",
    "print(basic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most likely distribution for each factor\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols = 4, figsize = (15,10))\n",
    "rw, col = 0,0\n",
    "for i in range(12):\n",
    "    fign = ax[rw,col]\n",
    "    fign.hist(data1.iloc[:,i], density = True)\n",
    "    fign.set_title(data1.columns[i])\n",
    "    \n",
    "    rw += 1\n",
    "    rw %= 3\n",
    "    col += rw == 0\n",
    "    \n",
    "fig.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "fig.suptitle(\"Histogram & Distribution\", fontsize = 25)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calculates various statistics\n",
    "def stat_cal(df, i, split_ratio, calcor = True, drop = True):\n",
    "    cori = -2\n",
    "    if calcor:\n",
    "        ex = df[i].mean()\n",
    "        ey = df[\"MEDV\"].mean()\n",
    "        ox = np.sqrt(sum([(x - ex)**2 for x in df[i]]))\n",
    "        oy = np.sqrt(sum([(y - ey)**2 for y in df[\"MEDV\"]]))\n",
    "        cori = np.sum([(x*y - ex*ey)/(ox*oy) for (x, y) in zip(df[i], df[\"MEDV\"])])\n",
    "        \n",
    "    if drop:\n",
    "        df = df.drop(i, axis = 1)\n",
    "        \n",
    "    n = int(df.shape[0]*split_ratio)\n",
    "    Xtrain = df.iloc[n:].drop(\"MEDV\", axis = 1)\n",
    "    Xtrain.insert(0, 'ONE', [1]*(df.shape[0] - n))#don't include test cases\n",
    "    Ytrain = df.loc[n:,\"MEDV\"]\n",
    "    \n",
    "    Xtest = df.iloc[:n,:].drop(\"MEDV\", axis = 1)\n",
    "    Xtest.insert(0, 'ONE', [1]*(n))#don't include test cases\n",
    "    Ytest = df.loc[:n-1,\"MEDV\"]\n",
    "    \n",
    "    theta = np.linalg.inv(Xtrain.T.dot(Xtrain)).dot(Xtrain.T).dot(Ytrain)\n",
    "\n",
    "    SSE = sum((Xtest.dot(theta) - Ytest)**2)\n",
    "    SST = sum((Ytest - np.mean(Ytest))**2)\n",
    "    R2 = 1 - SSE/SST\n",
    "    R2adj = 1 - (SSE/(Xtrain.shape[0] - Xtrain.shape[1] - 1))/(SST/(Xtrain.shape[0] - 1))#adjust both SSE,SST for df\n",
    "        \n",
    "    return SSE, SST, R2, R2adj, cori\n",
    "SSE, SST, R2, R2adj, cori = stat_cal(data1, \"LSTAT\", 0.2, calcor = False)\n",
    "print(SSE, SST, R2, R2adj, cori)#a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "corrs = data.corr()[\"MEDV\"]\n",
    "corr_indx = corrs.apply(abs).sort_values(ascending = False)[1:].index#drops the \"MEDV\", the y\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(corr_indx[0], corr_indx[1], c = \"MEDV\", data = data, alpha = 0.4, cmap=plt.get_cmap(\"gist_ncar\"))\n",
    "plt.colorbar().set_label(\"MEDV in $'000\")\n",
    "plt.xlabel(corr_indx[1], fontsize = 12)\n",
    "plt.ylabel(corr_indx[0], fontsize = 12)\n",
    "plt.title(\"Highest Correlated Vars vs. MEDV\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 3, ncols = 4, figsize = (15,10))\n",
    "rw_loc = 0\n",
    "col_loc = 0 \n",
    "\n",
    "data_SSE, data_SST = stat_cal(data, i, 0.2, calcor = False, drop = False)[:2]\n",
    "\n",
    "for i in corr_indx[:-1]:#eleven factors\n",
    "    #print(rw_loc, col_loc)\n",
    "    #print(ax[rw_loc, col_loc].plot(list(range(10)), list(range(10))))\n",
    "    fign = ax[rw_loc, col_loc] \n",
    "    fign.scatter(data[i], data[\"MEDV\"], alpha = 0.2)\n",
    "    fign.axes.get_xaxis().set_visible(True)\n",
    "    fign.axes.get_yaxis().set_visible(True)\n",
    "    fign.set_ylabel('MEDV')\n",
    "    fign.set_yticklabels([])\n",
    "    fign.frame_on = False\n",
    "    fign.set_title(i+ \" vs. $$$\", fontsize = 14)\n",
    "    fign.text(0.5, -0.2, 'corr: ' + str(\"%.3f\" % corrs[i]), horizontalalignment='center',\n",
    "              verticalalignment='center', transform=fign.transAxes)\n",
    "    fign.text(0.5, -0.3, 'Uniquely Explained Variance', horizontalalignment='center',\n",
    "              verticalalignment='center', transform=fign.transAxes)\n",
    "    line = sm.OLS(data[\"MEDV\"],sm.add_constant(data[i])).fit()\n",
    "    fign.plot(data[i], line.params[0] + line.params[1]*data[i],'r')\n",
    "    #next step is to add the amount of unique variance explained by this factor\n",
    "    SSE, SST, R2, R2adj, cori = stat_cal(data, i, 0.2, drop = True)\n",
    "    iSSE, iSST = stat_cal(data.loc[:,[i, \"MEDV\"]], i, 0.2, drop = False)[:2]\n",
    "    \n",
    "    #math below is wrong, need to update\n",
    "    #print(\"Variance explained: \", cori)#variance explained by everthing(prediction on all others + noise) but this factor/total error\n",
    "    #print(\"Uniquly explain % of total explained Variance\", ((data_SST - data_SSE) - SSE)/(data_SST))\n",
    "    #print(i, R2, R2adj, SSE, SST, cori, \"\\n\")\n",
    "    \n",
    "    col_loc += 1\n",
    "    col_loc %= 4\n",
    "    if col_loc == 0:\n",
    "        rw_loc += 1\n",
    "fig.subplots_adjust(hspace = 0.75)\n",
    "fig.suptitle('Plot of Attributes vs. Home Value', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how being along the chase river changes the price distribution? Graphically\n",
    "#next plot pdfs of (feature, price). 3d\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(data.iloc[:,1], data.iloc[:,2])\n",
    "#how to calculate the pdf of this function\n",
    "i = 'LSTAT'\n",
    "pnts_d = [[x, y] for x,y in zip(data.loc[:,i], data.loc[:,\"MEDV\"])]\n",
    "#print(pnts_d)\n",
    "#doesn't work yet either\n",
    "joint_prob, edges = np.histogramdd(np.array(data1[i], data1[\"MEDV\"]), bins = 3)\n",
    "\n",
    "#ax.scatter(data[:,i], data[:,\"MEDV\"], zs = data[\"MEDV\"], alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(3, 3)\n",
    "#list([data.iloc[:3,1], data.iloc[:3,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#how to include this in some pipeline?\n",
    "#######################ONLY run once\n",
    "cat_encoder = OneHotEncoder(sparse = False)\n",
    "cat_var_names = [\"RAD\",\"CHAS\"]\n",
    "cat_vars = data[cat_var_names]\n",
    "housing_cat1hot = cat_encoder.fit_transform(cat_vars)#represents 11 binary columns of \"YES/NO\" for a specific radial then chase river  \n",
    "\n",
    "cat_names = []\n",
    "for i in range(len(cat_encoder.categories_)):\n",
    "    for j in cat_encoder.categories_[i]:\n",
    "        cat_names.append(str(cat_var_names[i]) + str(int(j)))\n",
    "\n",
    "cats = pd.DataFrame(data = housing_cat1hot, columns = cat_names)\n",
    "data1 = pd.merge(data, cats, left_on=data1.index, right_on=cats.index,how='left')\n",
    "cat_var_names.append(\"key_0\")#gets included w/ merge\n",
    "data1.drop(labels = cat_var_names, axis = 1)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should make this a pipeline\n",
    "def cat_vars(lst):\n",
    "    out = []\n",
    "    for i in lst:\n",
    "        cat_vars= data1[[i]]\n",
    "        cat_encoder = OneHotEncoder(sparse = False)\n",
    "        housing_cat1hot = cat_encoder.fit_transform(cat_vars)\n",
    "        out.append(housing_cat1hot)\n",
    "    return(out)\n",
    "cat_atts = [\"RAD\", \"CHAS\"]\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([(\"cat\", OneHotEncoder(), cat_atts)])\n",
    "data2 = full_pipeline.fit_transform(data1)#just has catagorical variables? figure this out. Are the columns of arrays or many columns?\n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#do I need to keep a segment of data away from the rest perminantly?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y = data[\"MEDV\"]\n",
    "X = data.drop(\"MEDV\", axis = 1)\n",
    "X.insert(0,'ONE',[1]*X.shape[0])#prepend column of one's  for theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually calculate the best linear regression\n",
    "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "SSE = sum((y - X.dot(theta))**2)\n",
    "SST = sum((y - np.sum(y))**2)\n",
    "R2 = 1 - SSE/SST\n",
    "print(R2)#very good fit, but included too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "theta = np.linalg.inv(x_train.T.dot(x_train)).dot(x_train.T).dot(y_train)\n",
    "SSE = sum((y_test - x_test.dot(theta))**2)\n",
    "SST = sum((y_test - sum(y_test))**2)\n",
    "R2 = 1 - SSE/SST\n",
    "print(\"R2: \", R2)#this is >0.99999? why? regardless of how much I split my test data? need to adj R^2\n",
    "MSE = SSE/(X.shape[0] - X.shape[1])#added the theta0 parameter for MEDV which was taken out\n",
    "print(\"MSE: \", MSE)\n",
    "Radj = 1 - MSE/(np.std(y_test)**2)\n",
    "print(\"Radj\", Radj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatically build model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "reg = lin_reg.fit(x_train, y_train)\n",
    "print(\"reg score: \", reg.score(x_test, y_test))#why does this perform much worse than Radj?\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE: \", mean_squared_error(y_test, x_test.dot(reg.coef_) + reg.intercept_))\n",
    "np.mean((x_test.dot(reg.coef_) + reg.intercept_ - y_test)**2)\n",
    "#the reason you have to include reg.intercept_ despite having the column of 1's in X is that theta0\n",
    "#in the regession = 0, as that term has no impact on the final outcome of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forrest_scores = cross_val_score(forest_reg, x_train, y_train\n",
    "                                scoring = \"neg_mean_squared_error\", cv =10)\n",
    "forest_rsme_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
